{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ca9c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from audiocraft.models import MusicGen\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82486020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur1188/.conda/envs/myenv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer layers: 24\n"
     ]
    }
   ],
   "source": [
    "music_model = MusicGen.get_pretrained(\"facebook/musicgen-small\", device=device)\n",
    "music_model.set_generation_params(duration=4, use_sampling=False)\n",
    "music_model.compression_model.eval()\n",
    "music_model.lm.eval()\n",
    "print(f\"Transformer layers: {len(music_model.lm.transformer.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e86668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMModel(\n",
       "  (cfg_dropout): ClassifierFreeGuidanceDropout(p=0.3)\n",
       "  (att_dropout): AttributeDropout({})\n",
       "  (condition_provider): ConditioningProvider(\n",
       "    (conditioners): ModuleDict(\n",
       "      (description): T5Conditioner(\n",
       "        (output_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fuser): ConditionFuser()\n",
       "  (emb): ModuleList(\n",
       "    (0-3): 4 x ScaledEmbedding(2049, 1024)\n",
       "  )\n",
       "  (transformer): StreamingTransformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x StreamingTransformerLayer(\n",
       "        (self_attn): StreamingMultiheadAttention(\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (layer_scale_1): Identity()\n",
       "        (layer_scale_2): Identity()\n",
       "        (cross_attention): StreamingMultiheadAttention(\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout_cross): Dropout(p=0.0, inplace=False)\n",
       "        (norm_cross): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_scale_cross): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (linears): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(music_model.lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664d148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output norm: LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n"
     ]
    }
   ],
   "source": [
    "out_norm_layer = music_model.lm.out_norm\n",
    "print(f\"Output norm: {out_norm_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d797ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output norm: ModuleList(\n",
      "  (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear_layers = music_model.lm.linears\n",
    "print(f\"Output norm: {linear_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588a54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Literal, get_args\n",
    "\n",
    "MUSICGEN_SAMPLE_RATE, PARLER_TTS_SAMPLE_RATE = 32_000, 44_100\n",
    "_AUDIO_TYPES = Literal[\"music\", \"speech\"]\n",
    "\n",
    "def display_audio(samples: torch.Tensor, sample_rate: int = None, audio_type: str = None) -> None:\n",
    "    \"\"\"Renders an audio player for the given audio samples.\n",
    "\n",
    "    Args:\n",
    "        samples (torch.Tensor): a Tensor of decoded audio samples\n",
    "            with shapes [B, C, T] or [C, T]\n",
    "        sample_rate (int): sample rate audio should be displayed with.\n",
    "        audio_type (str): choose music or speech to set sample_rate to model\n",
    "            default sample rates.\n",
    "    \"\"\"\n",
    "    audio_options = get_args(_AUDIO_TYPES)\n",
    "    if audio_type != None:\n",
    "      assert audio_type in audio_options, f\"'{audio_type}' is not in {audio_options}\"\n",
    "    assert (sample_rate != None) != (audio_type != None), \\\n",
    "      f\" either sample_rate or audio_type has to be set but not both or neither, \\\n",
    "      sample_rate: '{sample_rate}' audio_type: '{audio_type}'\"\n",
    "    assert samples.dim() == 2 or samples.dim() == 3\n",
    "\n",
    "\n",
    "    if audio_type == 'music':\n",
    "      sample_rate = MUSICGEN_SAMPLE_RATE\n",
    "    elif audio_type == 'speech':\n",
    "      sample_rate = PARLER_TTS_SAMPLE_RATE\n",
    "\n",
    "    samples = samples.detach().cpu()\n",
    "    if samples.dim() == 2:\n",
    "        samples = samples[None, ...]\n",
    "\n",
    "    for audio in samples:\n",
    "        ipd.display(ipd.Audio(audio, rate=sample_rate))\n",
    "\n",
    "def display_music(samples: torch.Tensor) -> None:\n",
    "  display_audio(samples, audio_type='music')\n",
    "\n",
    "def display_speech(samples: torch.Tensor) -> None:\n",
    "  display_audio(samples, audio_type='speech')\n",
    "\n",
    "def display_wav_file(paths: str|list) -> None:\n",
    "  if isinstance(paths, list):\n",
    "    for path in paths:\n",
    "      ipd.display(ipd.Audio(path))\n",
    "  else:\n",
    "    ipd.display(ipd.Audio(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6badd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm output shape: torch.Size([2, 1024])\n",
      "torch.Size([2, 2048])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd\n",
    "\n",
    "x = torch.randn(2, 1024)\n",
    "\n",
    "def top_k_sample(logits, k=10):\n",
    "    values, indices = torch.topk(logits, k, dim=-1)\n",
    "    probs = F.softmax(values, dim=-1)\n",
    "    idx = torch.multinomial(probs, 1).squeeze(-1)\n",
    "    return torch.gather(indices, -1, idx.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    norm_out = music_model.lm.out_norm(x)\n",
    "    print(f\"Norm output shape: {norm_out.shape}\")\n",
    "    logits_out = [linear(norm_out) for linear in music_model.lm.linears]\n",
    "    print(logits_out[0].shape)\n",
    "\n",
    "\n",
    "    pred_tokens = [top_k_sample(logit) for logit in logits_out]\n",
    "    \n",
    "    codes = torch.stack(pred_tokens, dim=0).unsqueeze(0)\n",
    "    waveform = music_model.compression_model.decode(codes)\n",
    "\n",
    "display_audio(waveform, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6d879",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec58c46",
   "metadata": {},
   "source": [
    "## Baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aeb472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accdd51e",
   "metadata": {},
   "source": [
    "## Our model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00445b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing layer 0 for instrument piano\n",
      "self.X.shape=torch.Size([396, 1024]), self.Y.shape=torch.Size([396, 1024]), len(self.prompts)=396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/396 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 1, 1024]), y shape: torch.Size([1, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m norm_out \u001b[38;5;241m=\u001b[39m music_model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mout_norm(x)\n\u001b[1;32m     57\u001b[0m logits_out \u001b[38;5;241m=\u001b[39m [linear(norm_out) \u001b[38;5;28;01mfor\u001b[39;00m linear \u001b[38;5;129;01min\u001b[39;00m music_model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mlinears]\n\u001b[0;32m---> 58\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m [top_k_sample(logit) \u001b[38;5;28;01mfor\u001b[39;00m logit \u001b[38;5;129;01min\u001b[39;00m logits_out]\n\u001b[1;32m     60\u001b[0m codes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(pred_tokens, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m waveform \u001b[38;5;241m=\u001b[39m music_model\u001b[38;5;241m.\u001b[39mcompression_model\u001b[38;5;241m.\u001b[39mdecode(codes)\n",
      "Cell \u001b[0;32mIn[30], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m norm_out \u001b[38;5;241m=\u001b[39m music_model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mout_norm(x)\n\u001b[1;32m     57\u001b[0m logits_out \u001b[38;5;241m=\u001b[39m [linear(norm_out) \u001b[38;5;28;01mfor\u001b[39;00m linear \u001b[38;5;129;01min\u001b[39;00m music_model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mlinears]\n\u001b[0;32m---> 58\u001b[0m pred_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mtop_k_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m logit \u001b[38;5;129;01min\u001b[39;00m logits_out]\n\u001b[1;32m     60\u001b[0m codes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(pred_tokens, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     61\u001b[0m waveform \u001b[38;5;241m=\u001b[39m music_model\u001b[38;5;241m.\u001b[39mcompression_model\u001b[38;5;241m.\u001b[39mdecode(codes)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mtop_k_sample\u001b[0;34m(logits, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m values, indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(logits, k, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mgather(indices, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, idx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "from activations_dataset import ActivationsDataset\n",
    "from mlp_inferencing import MLP\n",
    "\n",
    "mlp_model = MLP(\n",
    "    input_dim=1024,\n",
    "    hidden_dim=2048,\n",
    "    output_dim=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "instruments = ['guitar', 'piano', 'trumpet', 'violin']\n",
    "instruments = ['piano']\n",
    "\n",
    "for layer in range(len(music_model.lm.transformer.layers) - 1):\n",
    "    for instrument in instruments:\n",
    "        print(f\"Testing layer {layer} for instrument {instrument}\")\n",
    "        \n",
    "        # Create the dataset for the current layer and instrument\n",
    "        test_set = ActivationsDataset(\n",
    "            data_dir='/home/scur1188/ai-intepr-project/data',\n",
    "            instruments=[instrument],\n",
    "            seeds=[1, 2, 3],\n",
    "            split=\"test\",\n",
    "            layer=layer,\n",
    "        )\n",
    "\n",
    "        pt_file = torch.load(\n",
    "            f\"/home/scur1188/ai-intepr-project/weights/layer_{layer:02d}_{instrument}_seeds-1,2,3_mlp.pt\", map_location=device)\n",
    "        mlp_model.load_state_dict(pt_file['model_state_dict'])\n",
    "        mlp_model.eval()\n",
    "        mlp_model.to(device)\n",
    "        music_model.lm.eval()\n",
    "        music_model.lm.to(device)\n",
    "        music_model.compression_model.to(device)\n",
    "        music_model.compression_model.eval()\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        # for i in tqdm(range(len(test_set))):\n",
    "        for batch in tqdm(test_loader):\n",
    "            # x, y, prompts = test_set[i]\n",
    "            x, y, prompts = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get the output from the MLP\n",
    "                output = mlp_model(x)\n",
    "\n",
    "                # Pass the input through the MusicGen model\n",
    "                norm_out = music_model.lm.out_norm(x)\n",
    "                logits_out = [linear(norm_out) for linear in music_model.lm.linears]\n",
    "                pred_tokens = [top_k_sample(logit) for logit in logits_out]\n",
    "\n",
    "                codes = torch.stack(pred_tokens, dim=0).unsqueeze(0)\n",
    "                waveform = music_model.compression_model.decode(codes)\n",
    "\n",
    "                # Display the waveform\n",
    "                display_audio(waveform, sample_rate=32000)\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ac6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor\n",
    "\n",
    "# # load data as prompts\n",
    "# x = 'Compose a happy classical song with a piano melody. Use a fast tempo.'\n",
    "\n",
    "# mlp_model = MLP(\n",
    "#     input_dim=1024,\n",
    "#     hidden_dim=2048,\n",
    "#     output_dim=1024,\n",
    "#     dropout=0.1\n",
    "# )\n",
    "# pt_file = torch.load(\n",
    "#             f\"/home/scur1188/ai-intepr-project/weights/layer_22_piano_seeds-1,2,3_mlp.pt\", map_location=device)\n",
    "# mlp_model.load_state_dict(pt_file['model_state_dict'])\n",
    "# mlp_model.eval()\n",
    "# mlp_model.to(device)\n",
    "# music_model = MusicGen.get_pretrained(\"facebook/musicgen-small\", device=device)\n",
    "# music_model.set_generation_params(duration=4)\n",
    "# music_model.compression_model.eval()\n",
    "# music_model.lm.eval() \n",
    "\n",
    "# original_first = music_model.lm.transformer.layers[:22]\n",
    "# music_model.lm.transformer.layers = nn.ModuleList([*original_first, mlp_model])\n",
    "# audio = music_model.generate([x])\n",
    "\n",
    "# music_model.lm.transformer.layers = nn.ModuleList([*original_first])\n",
    "# audio_decoder_lens = music_model.generate([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6677f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_music(audio)\n",
    "# display_music(audio_decoder_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hear21passt.base import load_model\n",
    "\n",
    "# music_classifier = load_model(mode=\"logits\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# piano_logit_index = 153\n",
    "# guitar_logit_index = 140\n",
    "# trumpet_logit_index = 187\n",
    "# violin_fiddle_logit_index = 191\n",
    "\n",
    "# logits = music_classifier(audio.squeeze(0).to(device))\n",
    "\n",
    "# piano_logit = logits[:, piano_logit_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Piano logit: {piano_logit.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba198db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.X.shape=torch.Size([396, 1024]), self.Y.shape=torch.Size([396, 1024]), len(self.prompts)=396\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset for the current layer and instrument\n",
    "test_set = ActivationsDataset(\n",
    "    data_dir='/home/scur1188/ai-intepr-project/data',\n",
    "    instruments=[instrument],\n",
    "    seeds=[1, 2, 3],\n",
    "    split=\"test\",\n",
    "    layer=layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e67533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
